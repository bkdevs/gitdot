---
title: "How we'll do CI"
date: "Feb 15 - Feb 22, 2026"
week: 8
---

There are times where life draws a blank. \
The days that become a blur, a blizzard of the ordinary.  \
The moments where we feel absent: not here, not there, not anywhere.

Today is one of those days.

I’m tired. \
And though I may feign ignorance to my fatigue, my body cannot deny it. \
We’ve been working a lot. But as strange as it is to say, the more I work, the less I remember what it is that I do.

Part of me wants to think that’s correct. \
More hours equates to more work, and more work means you can do things faster. \
That is a tempting default. In the face of uncertainty, the one thing you do know how to do is to move faster.

But part of me is finding reason to question it. \
More and more, engineering seems to be less about how quickly you can build a product, but more about whether you can build a better one.

It is much less clear how you do that. \
It is more creative, more speculative, more experimental — and it demands us to think differently than we have before.

It is a particular state of mind. \
One with enough courage to fight the fear of failure, but with enough freedom to enjoy the process and see where curiosity leads.

That might be corny to say, but I do believe it. \
Our attitude towards our work is reflected in the products we make. \
Anxiety leads to feature creep, burnout becomes bugs, and delusions are synonymous with defective design.

So you gotta love it. You gotta love what you do. \
You have to do it for more than the reason that you should.

This all means that I am likely overdue for a break, but anywho, here's CI.

&nbsp;

There are three things we set out to achieve with CI. \
I’ve alluded to them before, but to restate them here:

1. CI will be locally testable and reproducible
2. CI will be secure by design
3. CI will propose fixes for each identified error

At the time I wrote down these requirements, I had some vague notion that each would be possible to achieve, but wasn’t certain as to how. In the weeks that ensued, I spent a lot of time learning how CI works and was quite surprised at what I found.

For one, I had assumed that CI would be a push-based model. \
Events would come in, triggers would fire, work would get pushed to a queue, and then some cluster of workers would pick it up and execute as available.

That turned out to be wrong. \
Though some CI platforms are push-based (e.g. Jenkins, Github Actions), many others are pull-based (e.g., Gitlab, Woodpecker).

The difference between the two architectures usually comes down to latency, with real-time applications sensitive to delays largely favoring push. The benefit of pull-based architectures is that they’re typically simpler to operate and easier to scale. That is, they’re technically less complex. 
Things like periodic HTTP polling are “dumb”, but they work and they work well enough.

My gut reaction, as you might expect, was to try and do the best thing possible. \
I wanted to minimize time-to-job-start, the time from commit pushed to CI running. \
I wanted to make it 0 (or more realistically, say p99 @ 100ms).

With a pull-based architecture, you typically incur some delay from polling intervals (e.g., 3 seconds for woodpecker, 5 seconds for Gitlab). 
So that alone almost ruled out the paradigm for me.

But I didn’t get particularly far into designing a push-based orchestrator before realizing why both Gitlab and Woodpecker adopted pull-based models.

The reason is self-hosting. \
With a pull-based orchestrator, individual CI runners (i.e., the machines that actually execute tasks in a workflow) continuously poll for work from the main server. Every few seconds, they ask the main server where there is any work and if so, go ahead and execute.

The key advantage is security.  \
There is only one endpoint that needs to be exposed publicly in the main server — which also means that authentication is centralized, as opposed to having to authenticate requests from a single server to many self-hosted runners.

And it wasn’t so clear to me a few weeks ago, but self-hosting CI _is_ a primary use-case. 

For one, we think it’s the right thing to do.  \
For two, we think customers want it. \
And for three, there’s certain limitations that self-hosting solves.

Surprisingly, speed is one. \
I don’t know for sure, but I think it’s likely that Github’s managed actions runners are some sort of Lambda-esque infrastructure that spins up lightweight Firecracker VMs for each runner. VMs, as lightweight as you may make them, are costly to run.

You might ask why don’t they just use containers instead, but CI poses a challenge to containers as many builds want to build Docker images. That breaks security, as if each CI job was a container, it would need to mount the root Docker socket to build images, the equivalent of root access on a machine.

If you think about it, this managed fleet is quite the engineering ordeal. \
You are running hundreds of thousands of CI jobs across public and private repositories of all shapes and sizes, you need to ensure security isolation between each job, and you still want this to be reliable and fast.

I think that just doesn’t work. \
But I do know what does: just run it on your own machine.

If your CI only ever runs on your machine, you can do a lot of things. \
You can run it on bare-metal. You don’t need that VM, you get rid of that overhead for free.
You can cache, memoize, and do whatever strikes your fancy to make your builds as fast as humanly possible.
You can even use your own binaries (and credentials) as it’ll only ever be _your_ builds that run on that machine.

Of course, not everyone will want to self-host their own runner, and that’s okay too, we’re the sort of people to get excited by perverse engineering challenges, but regardless, I do think there are certain advantages that self-hosting will always have — and we plan to fully realize them.

And as a bit of engineering machismo, Claude and I prototyped some awfully hacky redis pub/sub connection pooling, so we’ll get rid of the delay and aim for a time-to-job-start of a 100ms or less too.


&nbsp;

Second, we’re fixing testing.

It is appalling that the industry norm to test CI is to push untested code to a separate branch, twiddle your fingers for whoever knows how long, get alerted of a whitespace error and rinse and repeat.

And to my knowledge, every single CI platform makes you do this. Yes they have linters, and yes there are ways to mitigate, but fundamentally there is no way to test CI end-to-end without pushing it to a branch.

You might ask, why not use [act](https://github.com/nektos/act), an open-source tool that lets you run GitHub actions locally? And my answer is that act is exactly what made me think that it should be possible to do better.

act _is_ a CI platform. They reverse-engineered GitHub actions, including all the random nuances as to what binaries are or are not included in the default images, and wrote an impressive Go orchestrator and runner that simulates and entire workflow locally. 

It is CI, there’s no real reason you couldn’t just take it and use it as Github action’s backend, in which case, CI would be fully locally testable — as the code you’re running locally and remotely are now the same.

But that’s not the case. \
As faithful of a reproduction as act is, it isn’t the same as Github’s own orchestrator, and it _never will be_ as long as the two are developed separately.

The solution is open-source. \
The code that we will run in our back-end will be the same as the code will share publicly, and on top of that, that same code will be exposed as a separate set of commands in our CLI. That’ll make it possible for anyone working in a gitdot repo to run CI locally (i.e., `gitdot run ci`).

There’s some complexity that I’m abstracting over here that I’ll address. \
The above implies that running the same binary on two systems should lead to the same set of results. That is rarely true in practice, as your laptop might be running a global Python version of 3.11, but your homelab might be running 3.9.

But it is a heavily studied problem, one that we should be wise enough to learn from those before us.

Docker is the primary solution that people point to. \
Containers are a process-isolated self-contained runnable, and though there are a few rare exceptions, running the same container on any environment should lead the same results.

But this is perhaps more my personal peeve than anything else, but I don’t like developing with Docker.
I have a small MacBook Pro with 256GB where I tend to do the bulk of my development and I dislike running Docker Desktop heavily as it saps battery and disk space. It also just ruins developer ergonomics more generally: a lot of things like hot reload and debuggers just don’t work that well (or if they do work, require effort that I am ill disposed to do) when running developer environments in a container.

That is to say, I view containers as the solution for the deployment of code, but I avoid them in the developing of code.

The reason I bring that up is that if you don’t enforce consistency between your local developer environment and the rest of your pipeline (e.g., CI), you again introduce a point of discrepancy. It may work fine in your local environment, but it may fail in CI as the dependencies in Docker are not the same as your local shell. So unless you fully commit to Dockerizing everything, you still don’t fully solve the problem of CI.

We’re going to go with Nix (and Docker). \
Nix is basically package lock files for every single system binary. 

By defining the complete set of inputs that are required for a build (i.e., commit sha and integrity hashes), you are able to consistently reproduce the exact binary at a given point in time. It’s a function, a pure function as defined with no side effects. This also means that the binaries themselves can be heavily cached as you should only ever really need to build a binary once.

There’s many more things that Nix does (including taking this concept and building an operating system with it), but from my understanding of it now, I do think it’s purpose is to solve the problem of reproducibility. If everyone on your team uses Nix, that means that every single binary, every single dependency you use (be it on your laptop, a coworker’s, a CI pipeline or even Prod) are all exactly the same.

The downside of Nix (again, personal) is that its syntax is quite hard to learn. \
There’s quite a lot of concepts that Nix introduces and it isn’t an easy endeavor to setup a Nix environment that you could use and develop in. Once you do, however, because Nix isn’t a VM (it’s just a shell with it’s own paths), nothing else in the developer experience has to change.

So we see our role as the following: make Nix easy to use.

This is a little less fleshed out than the above, but the short of it is that we’re thinking to use the user’s local environment as a source of truth when initializing a Nix environment. And I don’t know whether I want to put a wager on it now, but I think you can do this automatically and deterministically (yes, that means, no AI involved).


&nbsp;

Third, we’re going to be secure by design.

GitHub actions is a frequent target of attacks online. \
The incentive as to why is pretty clear: there are few attack vectors more desirable than centralized access to the world’s code. So unfortunately, it does get compromised. Some vulnerabilities are more severe with large repercussions and many are smaller in scale, but still affect thousands of users too.

It’s very hard to secure GitHub actions. \
The reason why is because it’s incredibly federated, both from the perspective of the GitHub actions marketplace (an a la carte pick your shell commands to run) as well as the pull requests that they run on. 

If you accidentally install a malicious action (or one with a CVE), you may have your secrets dumped to some S3 bucket and sold shortly thereafter. Likewise, a bad actor can also open a PR in a public repository that writes code to do the same, taking advantage of the fact that CI builds do need to run code.

Our thinking to the former is simple: let’s just disallow it. \
There will be no actions marketplace. 

Our plan is to build a few primitives into CI for the most frequently used (e.g., actions/checkout) and then to rely on our customers to write the rest. Especially now, it's unclear how much value there is in a pre-built CI script given you can just ask Claude to write a bespoke one for you.

As for PRs, we will use the author’s credentials to populate secrets in CI as opposed to defining secrets for each repository. That means that a maintainer of a repository will get a different set of secrets than an external contributor would, and that the external contributor would need to provide his own secrets (e.g., Anthropic API key) to run CI.

That begins to touch upon less fleshed out ideas on contribution models, so I’ll leave ourselves some ambiguity there.



&nbsp;

I was planning on writing a fourth, but this post seems a bit too long already and I am feeling like writing some code, so I’ll leave it at that. 

Thank you. \
—baepaul.
